{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Assignment\n",
    "\n",
    "It is totally prohibited to use any kind of loop. You can use stackoverflow. If you copy codes from previous answers, explain each step. No explanation is `0 points`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dictionaries\n",
    "1. Create a dictionary with two keys: `even_numbers` and `odd_numbers`. The first key should have all the even numbers in this range `[0, 2000]`, and the second key must have all the odd numbers in this range `[9000, 19000]`. The values should be stored in a `list`. **Hint: Use the `np.arange`, `zip`, and `np.tolist()` functions.** <br><br>\n",
    "2. Print the value of `brand` key of `car` dictionary. **Hint: Use the `get` method.** <br><br>\n",
    "3. Print all the the values of `brand` key of `car` dictionary. **Hint: Use the `values` method.** <br><br>\n",
    "4. Print the max value  of `friday` in `january` of `hr_sleep` dictionary. **Hint: [Indexing in nested dictioanries](https://stackoverflow.com/questions/25836376/how-to-get-the-inner-indexes-of-a-nested-dictionary-in-python).** <br><br>\n",
    "5. Add `march` key to the `hr_sleep` dictionary using `week1` and `values2` Python lists. **Hint: Use `zip` function adn [this link](https://stackoverflow.com/questions/1024847/how-can-i-add-new-keys-to-a-dictionary).** <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "car = {\n",
    "  \"brand\": \"Ford\",\n",
    "  \"model\": \"Mustang\",\n",
    "  \"year\": 1964\n",
    "}\n",
    "\n",
    "hr_sleep = {\"january\": {\"wednesday\": 7,\n",
    "                      \"thursday\": 8,\n",
    "                      \"friday\": [2, 2, 1, 2]},\n",
    "          \"february\": {\"saturday\": 5,\n",
    "                       \"sunday\": 10,\n",
    "                       \"monday\": 8\n",
    "          }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "week1 = ['monday', 'sunday']\n",
    "values2 = [ [2, 3, 4 ] , 8, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Kevin"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Importing numpy package\n",
    "import numpy as np\n",
    "\n",
    "# Creating two lists of the even and odd numbers\n",
    "evenNumbers = np.arange(0,2001,2).tolist()\n",
    "oddNumbers = np.arange(9001,19002,2).tolist()\n",
    "\n",
    "# Creating the lists that will be used to zip and create the dictionary\n",
    "keys = [ \"Even_Numbers\" , \"Odd_Numbers\"]\n",
    "elements = [ evenNumbers , oddNumbers]\n",
    "\n",
    "# Creating the dictionary with the key and elements listed before\n",
    "dict1 = dict(zip(keys , elements))\n",
    "\n",
    "# Proving that elements of each key are lists.\n",
    "print(type(dict1.get(\"Even_Numbers\")))\n",
    "print(type(dict1.get(\"Odd_Numbers\")))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. In order to print the value of the \"brand\" key of car dictionary we need to use the following code:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'Ford'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.get(\"brand\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. César A"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "'Ford'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we use the values method to obtain all the elements related to every key of \"car\" dictionary. Then we transform it to a list in order to have a list with 3 elements, each of them are the elements of every key. The first element (index 0) is the element related to the \"brand\" key of \"car\" dictionary.\n",
    "list(car.values())[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Percy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# First we use the get method in order to get all the elements related to \"january\" key. Then we can notice that the element of \"january\" key is another dictionary, so we can use again the get method to obtain the element related to \"friday\" key. Finally, we use max function to obtain the maximum value of friday.\n",
    "\n",
    "maximo = max(hr_sleep.get(\"january\").get(\"friday\"))\n",
    "print(maximo)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Diana"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'january': {'wednesday': 7, 'thursday': 8, 'friday': [2, 2, 1, 2]}, 'february': {'saturday': 5, 'sunday': 10, 'monday': 8}, 'march': {'monday': [2, 3, 4], 'sunday': 8}}\n"
     ]
    },
    {
     "data": {
      "text/plain": "['january', 'february', 'march']"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we use week1 and values2 lists to create a new dictionary named march\n",
    "\n",
    "march = dict(zip(week1, values2))\n",
    "\n",
    "# Then we use the update method to hr_sleep dictionary in order to add march key and the march dictionary as the element related to march key.\n",
    "hr_sleep.update( {\"march\" : march} )\n",
    "print(hr_sleep)\n",
    "\n",
    "# Now we make sure that the key is correctly added.\n",
    "list(hr_sleep.keys())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Numpy\n",
    "1. Replace all the `even_numbers` in `np1` with 100. **Hint: Use `indexing` in arrys and [this filer](https://stackoverflow.com/questions/41638751/filtering-even-numbers-in-python).** <br><br>\n",
    "2. Create a 3x3 matrix with values ranging from 0 to 8. **Hint: Use `np.arange` and `np.reshape` method.** <br><br>\n",
    "3. Consider an array `Z = [1,2,3,4,5,6,7,8,9,10]`, generate an array `R = [ [ 1, 2, 3, 4], [ 2, 3, 4, 5 ], [ 3, 4, 5, 6 ], ..., [ 7, 8, 9, 10 ] ]`?.**Hint: Use `np.arange` and `np.concatenate`** <br><br>\n",
    "4. Create a 3x3x3 array with random values. **Hint: Use [`np.random.random`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.random.html).** <br><br>\n",
    "5. Comment why this expression `np.nan == np.nan` is False. **Hint: Use stackoverflow.** <br><br>\n",
    "6. Print the `mean` and `standard deviation` of `np2`. **Hint: Use `f-string`, `np.mean`, and `np.var` methods.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np1 = np.arange( 200, 750)\n",
    "np2 = np.random.normal( 50 , 4, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. César A."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 201 100 203 100 205 100 207 100 209 100 211 100 213 100 215 100 217\n",
      " 100 219 100 221 100 223 100 225 100 227 100 229 100 231 100 233 100 235\n",
      " 100 237 100 239 100 241 100 243 100 245 100 247 100 249 100 251 100 253\n",
      " 100 255 100 257 100 259 100 261 100 263 100 265 100 267 100 269 100 271\n",
      " 100 273 100 275 100 277 100 279 100 281 100 283 100 285 100 287 100 289\n",
      " 100 291 100 293 100 295 100 297 100 299 100 301 100 303 100 305 100 307\n",
      " 100 309 100 311 100 313 100 315 100 317 100 319 100 321 100 323 100 325\n",
      " 100 327 100 329 100 331 100 333 100 335 100 337 100 339 100 341 100 343\n",
      " 100 345 100 347 100 349 100 351 100 353 100 355 100 357 100 359 100 361\n",
      " 100 363 100 365 100 367 100 369 100 371 100 373 100 375 100 377 100 379\n",
      " 100 381 100 383 100 385 100 387 100 389 100 391 100 393 100 395 100 397\n",
      " 100 399 100 401 100 403 100 405 100 407 100 409 100 411 100 413 100 415\n",
      " 100 417 100 419 100 421 100 423 100 425 100 427 100 429 100 431 100 433\n",
      " 100 435 100 437 100 439 100 441 100 443 100 445 100 447 100 449 100 451\n",
      " 100 453 100 455 100 457 100 459 100 461 100 463 100 465 100 467 100 469\n",
      " 100 471 100 473 100 475 100 477 100 479 100 481 100 483 100 485 100 487\n",
      " 100 489 100 491 100 493 100 495 100 497 100 499 100 501 100 503 100 505\n",
      " 100 507 100 509 100 511 100 513 100 515 100 517 100 519 100 521 100 523\n",
      " 100 525 100 527 100 529 100 531 100 533 100 535 100 537 100 539 100 541\n",
      " 100 543 100 545 100 547 100 549 100 551 100 553 100 555 100 557 100 559\n",
      " 100 561 100 563 100 565 100 567 100 569 100 571 100 573 100 575 100 577\n",
      " 100 579 100 581 100 583 100 585 100 587 100 589 100 591 100 593 100 595\n",
      " 100 597 100 599 100 601 100 603 100 605 100 607 100 609 100 611 100 613\n",
      " 100 615 100 617 100 619 100 621 100 623 100 625 100 627 100 629 100 631\n",
      " 100 633 100 635 100 637 100 639 100 641 100 643 100 645 100 647 100 649\n",
      " 100 651 100 653 100 655 100 657 100 659 100 661 100 663 100 665 100 667\n",
      " 100 669 100 671 100 673 100 675 100 677 100 679 100 681 100 683 100 685\n",
      " 100 687 100 689 100 691 100 693 100 695 100 697 100 699 100 701 100 703\n",
      " 100 705 100 707 100 709 100 711 100 713 100 715 100 717 100 719 100 721\n",
      " 100 723 100 725 100 727 100 729 100 731 100 733 100 735 100 737 100 739\n",
      " 100 741 100 743 100 745 100 747 100 749]\n"
     ]
    }
   ],
   "source": [
    "# First we use the indexing filter (np1%2==0) in order to get every item of np1 array that is even, then we replace them it to 100.\n",
    "np1[(np1%2==0)]=100\n",
    "print(np1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. In order to create a 3x3 matrix with values ranging from 0 to 8 we need to use first the `np.arange` function to create a vector from 0 to 8 elements. Then, we apply the `np.reshape` method to transform the vector to a 3x3 matrix. <br><br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "# Here we are creating first the vector 1x9, then reshaping it to a 3x3 matrix\n",
    "mat1 = np.arange(0,9,1).reshape(3,3)\n",
    "print(mat1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. César A."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. Consider an array `Z = [1,2,3,4,5,6,7,8,9,10]`, generate an array `R = [ [ 1, 2, 3, 4], [ 2, 3, 4, 5 ], [ 3, 4, 5, 6 ], ..., [ 7, 8, 9, 10 ] ]`?.**Hint: Use `np.arange` and `np.concatenate`** <br><br>\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Percy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[0.42134291, 0.97512838, 0.33169394],\n        [0.95309505, 0.36592932, 0.89897696],\n        [0.52752119, 0.07810298, 0.9714482 ]],\n\n       [[0.95525919, 0.27137073, 0.80599852],\n        [0.78837949, 0.37753449, 0.8754653 ],\n        [0.84903607, 0.34229196, 0.84417018]],\n\n       [[0.49036975, 0.90213403, 0.9038224 ],\n        [0.96683105, 0.18105741, 0.8824089 ],\n        [0.29686131, 0.11123568, 0.85538681]]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random((3,3,3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. np.nan == np.nan is False because you can't equalize two values that does not exist. So if you try to do a mathematical or logical operation like equalize, the output will be False."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "nan"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Comment why this expression `np.nan == np.nan` is False. **Hint: Use stackoverflow.** <br><br>\n",
    "\n",
    "np.nan == np.nan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Diana"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "'The mean of np2 is 49.88154376990469 and the standard deviation is 3.7009263660271396'"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Print the `mean` and `standard deviation` of `np2`. **Hint: Use `f-string`, `np.mean`, and `np.var` methods.**\n",
    "mean = np.mean(np2)\n",
    "sd = np.sqrt(np.var(np2))\n",
    "\n",
    "f'The mean of np2 is {mean} and the standard deviation is {sd}'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Least Squares\n",
    "\n",
    "Given the following equation:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \n",
    "lnCT &= \\beta_{0}+\\beta_{q}lnq+ \\frac{1}{2}\\beta_{qq} lnq^2+\\beta_{q1}lnqlnp_1+\\beta_{q2}lnqlnp_2+ \\beta_{q3}lnqlnp_{3} +\\beta_{1}lnp_1+\\beta_{2}lnp_2+ \\beta_{3}lnp_3 \\\\\n",
    "& + \\frac{1}{2}\\beta_{11}ln^{2}p_1+ \\frac{1}{2}\\beta_{22}ln^{2}p_{2}+ \\frac{1}{2}\\beta_{33}ln^{2}p_{3} + \\frac{1}{2}\\beta_{12}lnp_{1}lnp_{2}+ \\frac{1}{2}\\beta_{13}lnp_{1}lnp_{3}+\\frac{1}{2}\\beta_{23}lnp_{2}lnp_{3}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "ST: \n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\begin{aligned} \n",
    "\\beta_{1} + \\beta_{2} + \\beta_{3} &= 1 \\\\\n",
    "\\beta_{q1} + \\beta_{q2} + \\beta_{q3} &= 0 \\\\\n",
    "\\beta_{11} + \\beta_{12} + \\beta_{13} &= 0 \\\\\n",
    "\\beta_{21} + \\beta_{22} + \\beta_{23} &= 0 \\\\\n",
    "\\beta_{31} + \\beta_{32} + \\beta_{33} &= 0 \\\\\n",
    "\\beta_{ij} = \\beta_{ji}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Get $\\widehat{\\boldsymbol{\\beta}}^{(RLS)}$ vector from the equation bellow. Use the `q`, `p1`, `p2`, `p3`, `CT` numpies. <br><br>\n",
    "2. Get the `covariance matrix` $\\mathbb{V}{\\rm ar} \\left(\\widehat{\\boldsymbol{\\beta}}^{(RLS)} \\right)$. <br><br>\n",
    "**Hint: For more information about Restricted Least Squares [click here](http://web.vu.lt/mif/a.buteikis/wp-content/uploads/PE_Book/4-4-Multiple-RLS.html).**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "greene = pd.read_csv(r\"../../_data/christensen_greene_f4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = greene.COST.values\n",
    "q = greene.Q.values\n",
    "p1 = greene.PL.values\n",
    "p2 = greene.PF.values\n",
    "p3 = greene.PK.values"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. In order to obtain the $\\widehat{\\boldsymbol{\\beta}}^{(RLS)}$ vector we need to estimate the following equation:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\n",
    "\\widehat{\\beta}^{(RLS)} =  \\widehat{\\beta}^{(OLS)} - (X'X)^{-1}L'(L(X'X)^{-1}L')^{-1}(L\\widehat{\\beta}^{(OLS)}-r)\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\n",
    "\\widehat{\\beta}^{(OLS)} =   (X'X)^{-1}(X'Y)\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So first we need to obtain the X matrix, Y vector, L matrix and r vector. After that, we can estimate both $\\widehat{\\boldsymbol{\\beta}}^{(OLS)}$ and $\\widehat{\\boldsymbol{\\beta}}^{(RLS)}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First create the Y vector:\n",
    "Y = np.log(ct)\n",
    "print(Y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Then create the X matrix:\n",
    "X = np.column_stack([np.log(q), np.log(np.power(q,2))/2, np.log(q)*np.log(p1) , np.log(q)*np.log(p2), np.log(q)*np.log(p3),\n",
    "                     np.log(p1), np.log(p2) , np.log(p3) , np.power(np.log(p1),2)/2 , np.power(np.log(p2),2)/2 , np.power(np.log(p3),2)/2,\n",
    "                     np.log(p1)*np.log(p2)/2 , np.log(p1)*np.log(p3)/2 , np.log(p2)*np.log(p3)/2])\n",
    "print(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now create the L matrix\n",
    "L = np.vstack((np.concatenate((np.zeros(5) , np.ones(3) , np.zeros(6))),\n",
    "               np.concatenate((np.zeros(2),np.ones(3),np.zeros(9))),\n",
    "               np.concatenate((np.zeros(8),np.ones(1),np.zeros(2),np.ones(2),np.zeros(1))),\n",
    "               np.concatenate((np.zeros(9),np.ones(1),np.zeros(1),np.ones(1),np.zeros(1),np.ones(1))),\n",
    "               np.concatenate((np.zeros(10),np.ones(1),np.zeros(1),np.ones(2))),))\n",
    "print(L)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now create the r vector:\n",
    "r = np.concatenate((np.ones(1),np.zeros(4)))\n",
    "print(r)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now we have estimate the beta OLS\n",
    "b_ols = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
    "print(b_ols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.42685704e+15  3.42685564e+15  1.91165951e+02 -1.21944620e+02\n",
      " -6.92213315e+01  1.03428667e+03  7.46752423e+01 -1.10796191e+03\n",
      " -3.36794605e+02  6.39645326e+02  4.25101967e+02  6.11256229e+01\n",
      "  2.75668982e+02 -7.00770949e+02]\n"
     ]
    }
   ],
   "source": [
    "# Now we estimate the beta RLS\n",
    "b_rls = b_ols - np.linalg.inv(X.T @ X) @ L.T @ np.linalg.inv(L @ np.linalg.inv(X.T @ X) @ L.T) @ (L @ b_ols - r)\n",
    "print(b_rls)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To estimate the Var($\\widehat{\\boldsymbol{\\beta}}^{(RLS)}$), we need to estimate the following equations:\n",
    "\\begin{aligned}\n",
    "\n",
    "Var(\\widehat{\\beta}^{(RLS)}) =  \\sigma^2 (X'X)^{-1} - \\sigma^2 (X'X)^{-1} L'(L(X'X)^{-1}L')^{-1}L(X'X)^{-1}\n",
    "\\end{aligned}\n",
    "\n",
    "\\begin{aligned}\n",
    "\\widehat{\\sigma}_{RLS} = (\\widehat{\\epsilon}'\\widehat{\\epsilon})/(N - (k + 1 - M))\n",
    "\n",
    "\\end{aligned}\n",
    "\n",
    "\\begin{aligned}\n",
    "\\widehat{\\epsilon} = Y - X\\widehat{\\beta}_{RLS}\n",
    "\\end{aligned}\n",
    "\n",
    "So we need first to estimate the residuals errors, then estimate the variance of the residuals in order to obtain the estimate of Var($\\widehat{\\boldsymbol{\\beta}}^{(RLS)}$)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First we generate the residuals vector:\n",
    "\n",
    "e = Y - X @ b_rls\n",
    "print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# Now we estimate the variance of the RLS residuals:\n",
    "sigma2_rls = e.T @ e / (len(Y) - (np.shape(X)[1] + 1 - len(r)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.19903356e+43  7.19903356e+43 -3.70905149e+31  3.42427692e+31\n",
      "   2.84774569e+30 -1.45456802e+32  6.21091637e+32 -4.75634834e+32\n",
      "   2.05272215e+31  8.11367735e+32 -7.49311815e+32 -7.90603386e+32\n",
      "   7.70076164e+32 -2.07643493e+31]\n",
      " [ 7.19903356e+43 -7.19903356e+43  3.70905149e+31 -3.42427692e+31\n",
      "  -2.84774569e+30  1.45456802e+32 -6.21091637e+32  4.75634834e+32\n",
      "  -2.05272215e+31 -8.11367735e+32  7.49311815e+32  7.90603386e+32\n",
      "  -7.70076164e+32  2.07643493e+31]\n",
      " [-3.26276184e+31  3.26276184e+31 -1.26057507e+19  1.30056706e+19\n",
      "  -3.99919894e+17 -3.26605369e+19  2.72285672e+20 -2.39625135e+20\n",
      "  -0.00000000e+00  3.73815753e+20 -3.23865488e+20 -3.48840620e+20\n",
      "   3.48840620e+20 -2.49751322e+19]\n",
      " [ 1.90798361e+31 -1.90798361e+31  7.31630887e+18 -4.44727328e+18\n",
      "  -2.86903558e+18  1.64930280e+19 -1.79404453e+20  1.62911425e+20\n",
      "   0.00000000e+00 -2.16406237e+20  1.85499438e+20  2.00952838e+20\n",
      "  -2.00952838e+20  1.54533995e+19]\n",
      " [ 1.35477823e+31 -1.35477823e+31  5.28944187e+18 -8.55839735e+18\n",
      "   3.26895548e+18  1.61675089e+19 -9.28812193e+19  7.67137104e+19\n",
      "  -0.00000000e+00 -1.57409515e+20  1.38366050e+20  1.47887783e+20\n",
      "  -1.47887783e+20  9.52173266e+18]\n",
      " [-3.79446943e+32  3.79446943e+32 -1.62233116e+20  1.58429054e+20\n",
      "   3.80406187e+18 -3.40751179e+20  3.19761409e+21 -2.85686291e+21\n",
      "   0.00000000e+00  4.37299105e+21 -3.83875852e+21 -4.10587478e+21\n",
      "   4.10587478e+21 -2.67116265e+20]\n",
      " [ 6.96653488e+31 -6.96653488e+31  2.66857996e+19 -4.79313591e+19\n",
      "   2.12455595e+19  6.47210138e+19 -3.24054997e+20  2.59333983e+20\n",
      "   0.00000000e+00 -6.72055569e+20  6.12837573e+20  6.42446571e+20\n",
      "  -6.42446571e+20  2.96089978e+19]\n",
      " [ 3.09781594e+32 -3.09781594e+32  1.35547316e+20 -1.10497695e+20\n",
      "  -2.50496214e+19  2.76030166e+20 -2.87355909e+21  2.59752893e+21\n",
      "   0.00000000e+00 -3.70093548e+21  3.22592094e+21  3.46342821e+21\n",
      "  -3.46342821e+21  2.37507267e+20]\n",
      " [ 9.96074629e+31 -9.96074629e+31  4.20158860e+19 -4.19386757e+19\n",
      "  -7.72102942e+16  9.30623750e+19 -8.39492212e+20  7.46429837e+20\n",
      "   0.00000000e+00 -1.15371248e+21  1.00979940e+21  1.08175594e+21\n",
      "  -1.08175594e+21  7.19565398e+19]\n",
      " [-1.09019488e+32  1.09019488e+32 -5.00826155e+19  5.04892061e+19\n",
      "  -4.06590639e+17 -1.23843564e+20  1.05366647e+21 -9.29822904e+20\n",
      "   0.00000000e+00  1.44220336e+21 -1.25118860e+21 -1.34669598e+21\n",
      "   1.34669598e+21 -9.55073788e+19]\n",
      " [-9.45695933e+31  9.45695933e+31 -3.29844293e+19  3.18896273e+19\n",
      "   1.09480200e+18 -8.03611620e+19  7.03618015e+20 -6.23256853e+20\n",
      "  -0.00000000e+00  9.49388822e+20 -8.22779685e+20 -8.86084253e+20\n",
      "   8.86084253e+20 -6.33045684e+19]\n",
      " [-4.25787840e+31  4.25787840e+31 -1.24588499e+19  1.16695485e+19\n",
      "   7.89301466e+17 -2.47899865e+19  2.44721880e+20 -2.19931893e+20\n",
      "   0.00000000e+00  3.30448972e+20 -2.90695242e+20 -3.10572107e+20\n",
      "   3.10572107e+20 -1.98768647e+19]\n",
      " [-5.70286789e+31  5.70286789e+31 -2.95570361e+19  3.02691273e+19\n",
      "  -7.12091171e+17 -6.82723884e+19  5.94770332e+20 -5.26497944e+20\n",
      "   0.00000000e+00  8.23263509e+20 -7.19104159e+20 -7.71183834e+20\n",
      "   7.71183834e+20 -5.20796751e+19]\n",
      " [ 1.51598272e+32 -1.51598272e+32  6.25414654e+19 -6.21587546e+19\n",
      "  -3.82710827e+17  1.48633550e+20 -1.29838835e+21  1.14975480e+21\n",
      "  -0.00000000e+00 -1.77265233e+21  1.54188384e+21  1.65726809e+21\n",
      "  -1.65726809e+21  1.15384244e+20]]\n"
     ]
    }
   ],
   "source": [
    "# Now we can estimate the Variance of the RLS estimator\n",
    "var_b_rls = sigma2_rls * np.linalg.inv(X.T @ X ) - sigma2_rls * np.linalg.inv(X.T @ X) @ L.T @ np.linalg.inv(L @ np.linalg.inv(X.T @ X) @ L.T) @ L @ np.linalg.inv(X.T @ X)\n",
    "print(var_b_rls)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code bellow uncomment the second line to install savReaderWriter library. The `dict_varlabels` has the labels of the columns of rec1.\n",
    "\n",
    "1. Check if CASEID identifies each observation uniquely. **[Hint: Use is_unique method.](https://pandas.pydata.org/docs/reference/api/pandas.Series.is_unique.html)**\n",
    "2. Make the CASEID column the index of your data set. **[Hint: Use set_index](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html)**. \n",
    "3. Keep women who are 15-30 years old and live in Urban areas. **[Hint: Use query](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html)**.\n",
    "4. Generate a new column with the month of born. Just use the first three letters.  Use the English names of the months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read REC0111 data\n",
    "#!pip install savReaderWriter\n",
    "import savReaderWriter as sav\n",
    "rec1 = pd.read_spss( fr\"../../_data/endes/2019/REC0111.sav\" )\n",
    "\n",
    "# Get labels from sav file\n",
    "with sav.SavHeaderReader( fr\"../../_data/endes/2019/REC0111.sav\", ioUtf8=True) as header:\n",
    "    metadata = header.all()\n",
    "\n",
    "dict_varlabels = metadata[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'ID1': 'Año',\n 'HHID': 'Identificación Cuestionario del Hogar',\n 'CASEID': 'Identificación Cuestionario Individual',\n 'V001': 'Conglomerado',\n 'V002': 'Número de vivienda',\n 'V003': 'Número de línea de entrevistada',\n 'V004': 'Unidad de área final',\n 'V007': 'Año de la entrevista',\n 'V008': 'Fecha de la entrevista, Codificación centenaria de meses (CMC)',\n 'V009': 'Mes de nacimiento de la entrevistada',\n 'V010': 'Año de nacimiento de la entrevistada',\n 'V011': 'Fecha de nacimiento, Codificación centenaria de meses (CMC)',\n 'V012': 'Edad actual - entrevistada',\n 'V013': 'Edad actual por grupos de 5 años',\n 'V014': 'Integridad de la información para la fecha de nacimiento ',\n 'V015': 'Resultado entrevista individual',\n 'V017': 'Inicio del calendario, Codificación centenaria de mesesl CMC',\n 'V018': 'Columna del mes de la entrevista',\n 'V019': 'Duración del calendario',\n 'V019A': 'Número de columnas de calendario',\n 'V020': 'Muestra alguna vez casada',\n 'V021': 'Unidad de muestreo primario - conglomerado',\n 'V023': 'Dominio de ejemplo - Departamento',\n 'V024': 'Región',\n 'V025': 'Tipo de lugar de residencia',\n 'V026': 'El lugar de residencia en el que se entrevistó - De Facto',\n 'V027': 'Número de visitas',\n 'V028': 'Identificación del entrevistador',\n 'V029': 'Identificador del digitador',\n 'V030': 'Supervisor de campo',\n 'V031': 'Editor de campo',\n 'V032': 'Editor de la oficina',\n 'V033': 'Selección final del área de probabilidad',\n 'V034': 'Número de orden del esposo',\n 'V040': 'Altitud del conglomerado en metros',\n 'V042': 'Selección de hogar para hemoglobina',\n 'V043': 'Selección para módulo de estatus de mujeres',\n 'V044': 'Selección para módulo de violencia domestica',\n 'V000': 'Código y fase del país',\n 'Q105DD': 'Dia de nacimeinto de la entrevistada',\n 'V101': 'Región',\n 'V102': 'Tipo de lugar de residencia',\n 'V103': 'Lugar de residencia de la infancia',\n 'V104': 'Cuanto tiempo tiene viviendo continuamente en el lugar de residencia actual',\n 'V105': 'Tipo de lugar de residencia anteriormente',\n 'V106': 'Nivel educativo más alto',\n 'V107': 'Año/grado de educación más alto aprobado',\n 'V113': 'Fuente principal de abasteciemiento de agua potable que utilizan en su hogar para tomar o beber',\n 'V115': 'Tiempo para llegar a la fuente de agua',\n 'V116': 'Tipo de instalación sanitaria',\n 'V119': 'En su hogar tiene: electricidad',\n 'V120': 'En su hogar tiene: radio',\n 'V121': 'En su hogar tiene: televisión',\n 'V122': 'En su hogar tiene: refrigerador',\n 'V123': 'En su hogar tiene: bicicleta',\n 'V124': 'En su hogar tiene: motocicleta/motocar',\n 'V125': 'En su hogar tiene: coche/camión',\n 'V127': 'Material predominante del piso de la vivienda',\n 'V128': 'Material predominante de las paredes exteriores de la vivienda',\n 'V129': 'Material predominante del techo de la vivienda',\n 'V130': 'Religión',\n 'V131': 'Etnicidad',\n 'V133': 'Educación en años simples',\n 'V134': 'El lugar en el que se realizó la entrevista  De-facto',\n 'V135': 'Residente habitual o visitante',\n 'V136': 'Número de miembros del hogar',\n 'V137': 'Número de niños de 6 años de edad ',\n 'V138': 'Número de mujeres de 15 a 49 años de edad elegibles en el hogar ',\n 'V139': 'Región, residencia habitual De-jure',\n 'V140': 'Tipo de área de residencia De-jure',\n 'V141': 'Lugar de residencia De-jure',\n 'V149': 'Logro educativo',\n 'V150': 'Relación con el jefe del hogar',\n 'V151': 'Sexo del Jefe del Hogar',\n 'V152': 'Edad del jefe del hogar',\n 'V153': 'En su hogar tiene: teléfono',\n 'AWFACTT': 'Factor todas las mujeres - total',\n 'AWFACTU': 'Factor todas las mujeres - urbano/rural',\n 'AWFACTR': 'Factor todas las mujeres - regional',\n 'AWFACTE': 'Factor todas las mujeres - educación',\n 'AWFACTW': 'Factor todas las mujeres - índice de riqueza',\n 'V155': 'Alfabetización',\n 'V156': 'Alguna vez participó en un programa de alfabetización (no incluyendo la escuela primaria)',\n 'V157': 'Frecuencia de lectura de un periódico o revista',\n 'V158': 'Frecuencia de escuchar radio',\n 'V159': 'Frecuencia de ver televisión',\n 'V160': 'Baño compartido con otros hogares',\n 'V161': 'Tipo de combustible para cocinar',\n 'V166': 'Resultados de la prueba del yodo en la sal',\n 'V167': 'Número de viajes en los últimos 12 meses',\n 'V168': 'Afuera más de un mes en los últimos 12 meses',\n 'ML101': 'Tipo de mosquitero que utilizo para dormir última noche',\n 'QD333_1': 'Alguna dificultad o limitación permanente para ver, aún usando anteojos',\n 'QD333_2': 'Alguna dificultad o limitación permanente para oir, aún usando audífonos',\n 'QD333_3': 'Alguna dificultad o limitación permanente para hablar o comunicarse, aún usando la lengua de señas u otro',\n 'QD333_4': 'Alguna dificultad o limitación permanente para moverse o caminar para usar brazos y/o piernas',\n 'QD333_5': 'Alguna dificultad o limitación permanente para entender o aprender (concentrarse y recordarse)',\n 'QD333_6': 'Alguna dificultad o limitación permanente para relacionarse con los demás, por sus pensamientos, sentimientos, emociones o conductas',\n 'UBIGEO': 'Código de Ubicación Gegráfica',\n 'V022': 'Estratos',\n 'V005': 'Factor de ponderacion',\n 'V190': 'Índice de riqueza',\n 'V191': 'Factor de puntuación del índice de riqueza (5 decimales)',\n 'mujeres12a49': 'Mujeres de 12 a 49 años de edad',\n 'NCONGLOME': 'Número de Conglomerado (proveniente del marco)'}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_varlabels"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. In order to check if CASEID identifies each observation uniquely, we need to use the is_unique method after calling CASEID variable from rec1 dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The result is True which is right because CASEID is labeled as \"Identificación Cuestionario Individual\"\n",
    "rec1.CASEID.is_unique"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Percy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "                       ID1             HHID    V001  V002  V003    V004  \\\nCASEID                                                                    \n      000100201  2  2019.0        000100201     1.0   2.0   2.0     1.0   \n      000100201  3  2019.0        000100201     1.0   2.0   3.0     1.0   \n      000102801  2  2019.0        000102801     1.0  28.0   2.0     1.0   \n      000102801  6  2019.0        000102801     1.0  28.0   6.0     1.0   \n      000104801  2  2019.0        000104801     1.0  48.0   2.0     1.0   \n...                    ...              ...     ...   ...   ...     ...   \n      325406201  2  2019.0        325406201  3254.0  62.0   2.0  3254.0   \n      325406301  2  2019.0        325406301  3254.0  63.0   2.0  3254.0   \n      325407001  2  2019.0        325407001  3254.0  70.0   2.0  3254.0   \n      325407201  2  2019.0        325407201  3254.0  72.0   2.0  3254.0   \n      325407401  2  2019.0        325407401  3254.0  74.0   2.0  3254.0   \n\n                      V007    V008  V009    V010  ...  QD333_4  QD333_5  \\\nCASEID                                            ...                     \n      000100201  2  2019.0  1434.0   4.0  1986.0  ...       No       No   \n      000100201  3  2019.0  1434.0   1.0  2007.0  ...       No       No   \n      000102801  2  2019.0  1434.0   6.0  1983.0  ...       No       No   \n      000102801  6  2019.0  1434.0   3.0  1970.0  ...       No       No   \n      000104801  2  2019.0  1434.0   5.0  1991.0  ...       No       No   \n...                    ...     ...   ...     ...  ...      ...      ...   \n      325406201  2  2019.0  1440.0  12.0  1971.0  ...       No       No   \n      325406301  2  2019.0  1440.0   6.0  1988.0  ...       No       No   \n      325407001  2  2019.0  1440.0   7.0  1973.0  ...       No       No   \n      325407201  2  2019.0  1440.0  12.0  1994.0  ...       No       No   \n      325407401  2  2019.0  1440.0  10.0  1996.0  ...       No       No   \n\n                   QD333_6  UBIGEO   V022      V005          V190      V191  \\\nCASEID                                                                        \n      000100201  2      No  010101    3.0  154803.0          Rico  1.234450   \n      000100201  3      No  010101    3.0  154803.0          Rico  1.234450   \n      000102801  2      No  010101    3.0  154803.0          Rico  1.295611   \n      000102801  6      No  010101    3.0  154803.0          Rico  1.295611   \n      000104801  2      No  010101    3.0  154803.0        Pobrer -0.256431   \n...                    ...     ...    ...       ...           ...       ...   \n      325406201  2      No  250401  249.0  244995.0  El más pobre -1.750187   \n      325406301  2      No  250401  249.0  244995.0  El más pobre -1.676861   \n      325407001  2      No  250401  249.0  459792.0  El más pobre -1.585333   \n      325407201  2      No  250401  249.0  459792.0  El más pobre -1.650159   \n      325407401  2      No  250401  249.0  244995.0  El más pobre -1.644720   \n\n                                                     mujeres12a49 NCONGLOME  \nCASEID                                                                       \n      000100201  2                Mujeres de 15 a 49 años de edad    7065.0  \n      000100201  3  Mujeres de 12 a 14 de edad, nunca embarazadas    7065.0  \n      000102801  2                Mujeres de 15 a 49 años de edad    7065.0  \n      000102801  6                Mujeres de 15 a 49 años de edad    7065.0  \n      000104801  2                Mujeres de 15 a 49 años de edad    7065.0  \n...                                                           ...       ...  \n      325406201  2                Mujeres de 15 a 49 años de edad   15783.0  \n      325406301  2                Mujeres de 15 a 49 años de edad   15783.0  \n      325407001  2                Mujeres de 15 a 49 años de edad   15783.0  \n      325407201  2                Mujeres de 15 a 49 años de edad   15783.0  \n      325407401  2                Mujeres de 15 a 49 años de edad   15783.0  \n\n[38335 rows x 104 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID1</th>\n      <th>HHID</th>\n      <th>V001</th>\n      <th>V002</th>\n      <th>V003</th>\n      <th>V004</th>\n      <th>V007</th>\n      <th>V008</th>\n      <th>V009</th>\n      <th>V010</th>\n      <th>...</th>\n      <th>QD333_4</th>\n      <th>QD333_5</th>\n      <th>QD333_6</th>\n      <th>UBIGEO</th>\n      <th>V022</th>\n      <th>V005</th>\n      <th>V190</th>\n      <th>V191</th>\n      <th>mujeres12a49</th>\n      <th>NCONGLOME</th>\n    </tr>\n    <tr>\n      <th>CASEID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>000100201  2</th>\n      <td>2019.0</td>\n      <td>000100201</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2019.0</td>\n      <td>1434.0</td>\n      <td>4.0</td>\n      <td>1986.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>010101</td>\n      <td>3.0</td>\n      <td>154803.0</td>\n      <td>Rico</td>\n      <td>1.234450</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>7065.0</td>\n    </tr>\n    <tr>\n      <th>000100201  3</th>\n      <td>2019.0</td>\n      <td>000100201</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>2019.0</td>\n      <td>1434.0</td>\n      <td>1.0</td>\n      <td>2007.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>010101</td>\n      <td>3.0</td>\n      <td>154803.0</td>\n      <td>Rico</td>\n      <td>1.234450</td>\n      <td>Mujeres de 12 a 14 de edad, nunca embarazadas</td>\n      <td>7065.0</td>\n    </tr>\n    <tr>\n      <th>000102801  2</th>\n      <td>2019.0</td>\n      <td>000102801</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2019.0</td>\n      <td>1434.0</td>\n      <td>6.0</td>\n      <td>1983.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>010101</td>\n      <td>3.0</td>\n      <td>154803.0</td>\n      <td>Rico</td>\n      <td>1.295611</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>7065.0</td>\n    </tr>\n    <tr>\n      <th>000102801  6</th>\n      <td>2019.0</td>\n      <td>000102801</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>2019.0</td>\n      <td>1434.0</td>\n      <td>3.0</td>\n      <td>1970.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>010101</td>\n      <td>3.0</td>\n      <td>154803.0</td>\n      <td>Rico</td>\n      <td>1.295611</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>7065.0</td>\n    </tr>\n    <tr>\n      <th>000104801  2</th>\n      <td>2019.0</td>\n      <td>000104801</td>\n      <td>1.0</td>\n      <td>48.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2019.0</td>\n      <td>1434.0</td>\n      <td>5.0</td>\n      <td>1991.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>010101</td>\n      <td>3.0</td>\n      <td>154803.0</td>\n      <td>Pobrer</td>\n      <td>-0.256431</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>7065.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>325406201  2</th>\n      <td>2019.0</td>\n      <td>325406201</td>\n      <td>3254.0</td>\n      <td>62.0</td>\n      <td>2.0</td>\n      <td>3254.0</td>\n      <td>2019.0</td>\n      <td>1440.0</td>\n      <td>12.0</td>\n      <td>1971.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>250401</td>\n      <td>249.0</td>\n      <td>244995.0</td>\n      <td>El más pobre</td>\n      <td>-1.750187</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>15783.0</td>\n    </tr>\n    <tr>\n      <th>325406301  2</th>\n      <td>2019.0</td>\n      <td>325406301</td>\n      <td>3254.0</td>\n      <td>63.0</td>\n      <td>2.0</td>\n      <td>3254.0</td>\n      <td>2019.0</td>\n      <td>1440.0</td>\n      <td>6.0</td>\n      <td>1988.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>250401</td>\n      <td>249.0</td>\n      <td>244995.0</td>\n      <td>El más pobre</td>\n      <td>-1.676861</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>15783.0</td>\n    </tr>\n    <tr>\n      <th>325407001  2</th>\n      <td>2019.0</td>\n      <td>325407001</td>\n      <td>3254.0</td>\n      <td>70.0</td>\n      <td>2.0</td>\n      <td>3254.0</td>\n      <td>2019.0</td>\n      <td>1440.0</td>\n      <td>7.0</td>\n      <td>1973.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>250401</td>\n      <td>249.0</td>\n      <td>459792.0</td>\n      <td>El más pobre</td>\n      <td>-1.585333</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>15783.0</td>\n    </tr>\n    <tr>\n      <th>325407201  2</th>\n      <td>2019.0</td>\n      <td>325407201</td>\n      <td>3254.0</td>\n      <td>72.0</td>\n      <td>2.0</td>\n      <td>3254.0</td>\n      <td>2019.0</td>\n      <td>1440.0</td>\n      <td>12.0</td>\n      <td>1994.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>250401</td>\n      <td>249.0</td>\n      <td>459792.0</td>\n      <td>El más pobre</td>\n      <td>-1.650159</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>15783.0</td>\n    </tr>\n    <tr>\n      <th>325407401  2</th>\n      <td>2019.0</td>\n      <td>325407401</td>\n      <td>3254.0</td>\n      <td>74.0</td>\n      <td>2.0</td>\n      <td>3254.0</td>\n      <td>2019.0</td>\n      <td>1440.0</td>\n      <td>10.0</td>\n      <td>1996.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>250401</td>\n      <td>249.0</td>\n      <td>244995.0</td>\n      <td>El más pobre</td>\n      <td>-1.644720</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>15783.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>38335 rows × 104 columns</p>\n</div>"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Make the CASEID column the index of your data set. **[Hint: Use set_index](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html)**.\n",
    "\n",
    "rec1.set_index(\"CASEID\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. César A"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "          ID1             HHID              CASEID    V001   V002  V003  \\\n4      2019.0        000104801        000104801  2     1.0   48.0   2.0   \n8      2019.0        000116701        000116701  1     1.0  167.0   1.0   \n9      2019.0        000119101        000119101  1     1.0  191.0   1.0   \n12     2019.0        000203001        000203001  2     2.0   30.0   2.0   \n14     2019.0        000204001        000204001  3     2.0   40.0   3.0   \n...       ...              ...                 ...     ...    ...   ...   \n38162  2019.0        324202401        324202401  2  3242.0   24.0   2.0   \n38163  2019.0        324202501        324202501  3  3242.0   25.0   3.0   \n38164  2019.0        324203901        324203901  1  3242.0   39.0   1.0   \n38165  2019.0        324204501        324204501  2  3242.0   45.0   2.0   \n38170  2019.0        324206901        324206901  3  3242.0   69.0   3.0   \n\n         V004    V007    V008  V009  ...  QD333_4  QD333_5  QD333_6  UBIGEO  \\\n4         1.0  2019.0  1434.0   5.0  ...       No       No       No  010101   \n8         1.0  2019.0  1434.0   6.0  ...       No       No       No  010101   \n9         1.0  2019.0  1434.0   9.0  ...       No       No       No  010101   \n12        2.0  2019.0  1432.0   5.0  ...       No       No       No  010101   \n14        2.0  2019.0  1432.0   9.0  ...       No       No       No  010101   \n...       ...     ...     ...   ...  ...      ...      ...      ...     ...   \n38162  3242.0  2019.0  1436.0   2.0  ...       No       No       No  250302   \n38163  3242.0  2019.0  1436.0   5.0  ...       No       No       No  250302   \n38164  3242.0  2019.0  1436.0   8.0  ...       No       No       No  250302   \n38165  3242.0  2019.0  1436.0   5.0  ...       No       No       No  250302   \n38170  3242.0  2019.0  1436.0  10.0  ...       No       No       No  250302   \n\n        V022      V005          V190      V191  \\\n4        3.0  154803.0        Pobrer -0.256431   \n8        3.0  154803.0      Más rico  1.672440   \n9        3.0  154803.0        Pobrer  0.234148   \n12       2.0   65298.0        Pobrer -0.196449   \n14       2.0   65298.0         Medio  0.560448   \n...      ...       ...           ...       ...   \n38162  248.0  203603.0        Pobrer -0.463931   \n38163  248.0  567613.0        Pobrer -0.576118   \n38164  248.0  567613.0  El más pobre -0.940013   \n38165  248.0  203603.0  El más pobre -0.656011   \n38170  248.0  567613.0        Pobrer -0.574662   \n\n                          mujeres12a49  NCONGLOME  \n4      Mujeres de 15 a 49 años de edad     7065.0  \n8      Mujeres de 15 a 49 años de edad     7065.0  \n9      Mujeres de 15 a 49 años de edad     7065.0  \n12     Mujeres de 15 a 49 años de edad     7068.0  \n14     Mujeres de 15 a 49 años de edad     7068.0  \n...                                ...        ...  \n38162  Mujeres de 15 a 49 años de edad    45556.0  \n38163  Mujeres de 15 a 49 años de edad    45556.0  \n38164  Mujeres de 15 a 49 años de edad    45556.0  \n38165  Mujeres de 15 a 49 años de edad    45556.0  \n38170  Mujeres de 15 a 49 años de edad    45556.0  \n\n[11327 rows x 105 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID1</th>\n      <th>HHID</th>\n      <th>CASEID</th>\n      <th>V001</th>\n      <th>V002</th>\n      <th>V003</th>\n      <th>V004</th>\n      <th>V007</th>\n      <th>V008</th>\n      <th>V009</th>\n      <th>...</th>\n      <th>QD333_4</th>\n      <th>QD333_5</th>\n      <th>QD333_6</th>\n      <th>UBIGEO</th>\n      <th>V022</th>\n      <th>V005</th>\n      <th>V190</th>\n      <th>V191</th>\n      <th>mujeres12a49</th>\n      <th>NCONGLOME</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>2019.0</td>\n      <td>000104801</td>\n      <td>000104801  2</td>\n      <td>1.0</td>\n      <td>48.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>2019.0</td>\n      <td>1434.0</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>010101</td>\n      <td>3.0</td>\n      <td>154803.0</td>\n      <td>Pobrer</td>\n      <td>-0.256431</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>7065.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2019.0</td>\n      <td>000116701</td>\n      <td>000116701  1</td>\n      <td>1.0</td>\n      <td>167.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2019.0</td>\n      <td>1434.0</td>\n      <td>6.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>010101</td>\n      <td>3.0</td>\n      <td>154803.0</td>\n      <td>Más rico</td>\n      <td>1.672440</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>7065.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2019.0</td>\n      <td>000119101</td>\n      <td>000119101  1</td>\n      <td>1.0</td>\n      <td>191.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2019.0</td>\n      <td>1434.0</td>\n      <td>9.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>010101</td>\n      <td>3.0</td>\n      <td>154803.0</td>\n      <td>Pobrer</td>\n      <td>0.234148</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>7065.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2019.0</td>\n      <td>000203001</td>\n      <td>000203001  2</td>\n      <td>2.0</td>\n      <td>30.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2019.0</td>\n      <td>1432.0</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>010101</td>\n      <td>2.0</td>\n      <td>65298.0</td>\n      <td>Pobrer</td>\n      <td>-0.196449</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>7068.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2019.0</td>\n      <td>000204001</td>\n      <td>000204001  3</td>\n      <td>2.0</td>\n      <td>40.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2019.0</td>\n      <td>1432.0</td>\n      <td>9.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>010101</td>\n      <td>2.0</td>\n      <td>65298.0</td>\n      <td>Medio</td>\n      <td>0.560448</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>7068.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>38162</th>\n      <td>2019.0</td>\n      <td>324202401</td>\n      <td>324202401  2</td>\n      <td>3242.0</td>\n      <td>24.0</td>\n      <td>2.0</td>\n      <td>3242.0</td>\n      <td>2019.0</td>\n      <td>1436.0</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>250302</td>\n      <td>248.0</td>\n      <td>203603.0</td>\n      <td>Pobrer</td>\n      <td>-0.463931</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>45556.0</td>\n    </tr>\n    <tr>\n      <th>38163</th>\n      <td>2019.0</td>\n      <td>324202501</td>\n      <td>324202501  3</td>\n      <td>3242.0</td>\n      <td>25.0</td>\n      <td>3.0</td>\n      <td>3242.0</td>\n      <td>2019.0</td>\n      <td>1436.0</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>250302</td>\n      <td>248.0</td>\n      <td>567613.0</td>\n      <td>Pobrer</td>\n      <td>-0.576118</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>45556.0</td>\n    </tr>\n    <tr>\n      <th>38164</th>\n      <td>2019.0</td>\n      <td>324203901</td>\n      <td>324203901  1</td>\n      <td>3242.0</td>\n      <td>39.0</td>\n      <td>1.0</td>\n      <td>3242.0</td>\n      <td>2019.0</td>\n      <td>1436.0</td>\n      <td>8.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>250302</td>\n      <td>248.0</td>\n      <td>567613.0</td>\n      <td>El más pobre</td>\n      <td>-0.940013</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>45556.0</td>\n    </tr>\n    <tr>\n      <th>38165</th>\n      <td>2019.0</td>\n      <td>324204501</td>\n      <td>324204501  2</td>\n      <td>3242.0</td>\n      <td>45.0</td>\n      <td>2.0</td>\n      <td>3242.0</td>\n      <td>2019.0</td>\n      <td>1436.0</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>250302</td>\n      <td>248.0</td>\n      <td>203603.0</td>\n      <td>El más pobre</td>\n      <td>-0.656011</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>45556.0</td>\n    </tr>\n    <tr>\n      <th>38170</th>\n      <td>2019.0</td>\n      <td>324206901</td>\n      <td>324206901  3</td>\n      <td>3242.0</td>\n      <td>69.0</td>\n      <td>3.0</td>\n      <td>3242.0</td>\n      <td>2019.0</td>\n      <td>1436.0</td>\n      <td>10.0</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>250302</td>\n      <td>248.0</td>\n      <td>567613.0</td>\n      <td>Pobrer</td>\n      <td>-0.574662</td>\n      <td>Mujeres de 15 a 49 años de edad</td>\n      <td>45556.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>11327 rows × 105 columns</p>\n</div>"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Keep women who are 15-30 years old and live in Urban areas. **[Hint: Use query](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html)**.\n",
    "\n",
    "rec1.query(\"(V013 == 'De 15 a 19 años de edad' or V013 == 'De 20 a 24 años de edad' or V013 == 'De 25 a 29 años de edad') and (V025 == 'Urbano')\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Diana"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "# In order to obtain this new variable, we used the merge function with an auxiliary DataFrame that contains the months' names.\n",
    "\n",
    "# First, we generate the auxiliary DataFrame by creating a dictionary with the two lists.\n",
    "months_n = np.arange(1.0, 13.0, 1)\n",
    "months_l = [ 'jan' , 'feb', 'mar' , 'abr' , 'may' , 'jun' , 'jul', 'ago' , 'sep' , 'oct', 'nov', 'dec']\n",
    "\n",
    "dict_months = { \"V009\" : months_n, \"months\":months_l}\n",
    "month = pd.DataFrame(dict_months)\n",
    "\n",
    "# Now we use the merge function to add the month variable\n",
    "rec1_new = rec1.merge(month , on = 'V009' , how = 'left',  validate = 'm:1')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
